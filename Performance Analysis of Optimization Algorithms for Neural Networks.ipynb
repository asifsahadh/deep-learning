{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analysis of Optimization Algorithms for Neural Networks\n",
    "This notebook contains implementations of the Adam optimizer and the key algorithms prior to it. These include Gradient Descent, Momentum, Adagrad, and RMSprop. Further, existing variations of Adam, such as Adamax, AdaBelief & Nadam are implemented.<br><br>\n",
    "The goal is to minimize the **loss function of a neural network** with the following architecture, where each number represents the number of neurons in a layer: [4, 5, 5, 12, 1, 22, 2].\n",
    "\n",
    "Supporting materials:\n",
    "<br>\n",
    "_[Andrej Karpathy's repository](https://github.com/karpathy/micrograd/tree/master/micrograd) (micrograd)_\n",
    "<br>\n",
    "_[Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980) (original paper on Adam)_ \n",
    "<br>\n",
    "_[A journey into Optimization algorithms for Deep Neural Networks](https://theaisummer.com/optimization/) (a nice artice on the math behind neural network optimizers)_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of epochs: 73258\n",
      "Final loss: 1e-08\n"
     ]
    }
   ],
   "source": [
    "class NNGD:\n",
    "    def __init__(self, layer_sizes, learning_rate = 0.1):\n",
    "        np.random.seed(0)\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "\n",
    "            # initialize random weights and biases\n",
    "            w = np.random.randn(layer_sizes[i], layer_sizes[i + 1])\n",
    "            b = np.zeros((1, layer_sizes[i + 1]))\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "\n",
    "    # activation function\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # activation function gradient (for backpropagation)\n",
    "    def sigmoid_gradient(self, x):\n",
    "        sx = self.sigmoid(x)\n",
    "        return sx * (1 - sx)\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        self.activations = [X] # activation - input matrix\n",
    "        self.z_values = [] # z_values - intermediate matrix before activation (sigmoid) is applied\n",
    "\n",
    "        activation = X\n",
    "        # perform matrix multiplication (xw + b)\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(activation, w) + b\n",
    "            self.z_values.append(z)\n",
    "            activation = self.sigmoid(z)\n",
    "            self.activations.append(activation) \n",
    "\n",
    "        return activation # final activated output (sigmoid(xw + b))\n",
    "\n",
    "    def backpropagation(self, X, y):\n",
    "        m = X.shape[0] # training data size\n",
    "        delta = self.activations[-1] - y # error (y_cap - y)\n",
    "\n",
    "        for i in range(len(self.weights) - 1, -1, -1): # this loop starts from the last layer, all the way to the first layer\n",
    "            dW = np.dot(self.activations[i].T, delta) / m # derivative of error w.r.t weights\n",
    "            db = np.sum(delta, axis=0, keepdims=True) / m # derivative of error w.r.t biases\n",
    "\n",
    "            if i > 0: # we cannnot include the input however\n",
    "                delta = np.dot(delta, self.weights[i].T) * self.sigmoid_gradient(self.z_values[i - 1]) # error update \n",
    "\n",
    "            ##########################################\n",
    "            # GRADIENT DESCENT\n",
    "\n",
    "            # (w1 = w0 - (learning rate * (dL/dW)))\n",
    "            self.weights[i] -= self.learning_rate * dW\n",
    "\n",
    "            # (b1 = b0 - (learning rate * (dL/db)))\n",
    "            self.biases[i] -= self.learning_rate * db \n",
    "            \n",
    "            ##########################################\n",
    "\n",
    "    def train(self, X, y):\n",
    "        losses = []\n",
    "        epoch = 0\n",
    "        loss = 1e-8\n",
    "        while loss >= 1e-8:\n",
    "            output = self.forward_propagation(X) # perform forward propagation\n",
    "            loss = np.mean(np.square(output - y)) # calculate mean squared error loss\n",
    "            losses.append(loss)\n",
    "            self.backpropagation(X, y) # perform backpropagation   \n",
    "            epoch += 1        \n",
    "        print(f\"Total number of epochs: {epoch}\\nFinal loss: {loss.round(10)}\")\n",
    "\n",
    "# run neural network with gradient descent\n",
    "X = np.array([[5, 4, 2, -3],\n",
    "              [3, 1, -5, 0],\n",
    "              [9, -9, 3, 9],\n",
    "              [3, 1, -5, 0]])\n",
    "y = np.array([[1],\n",
    "              [0],\n",
    "              [0],\n",
    "              [0]])\n",
    "\n",
    "nn = NNGD((4, 5, 5, 12, 1, 22, 2))\n",
    "nn.train(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN with Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:**<br>\n",
    "1. There are 2 new parameters here; Velocity and Friction. Velocity is calculated as the running mean of gradients.<br>\n",
    "2. This forces the gradients to keep moving at each time step. Friction is just a constant that aims to decay this effect.<br>\n",
    "3. Basically, the function value at the previous time step is decayed slightly to give it less importance while giving full importance to the new function value (learning_rate * slope). This is saved into a variable. Next, this new variable will be decayed. Iteratively, a moving average (or smoothing) of previous function values are stored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of epochs: 415\n",
      "Final loss: 1e-08\n"
     ]
    }
   ],
   "source": [
    "class NNMomentum:\n",
    "    def __init__(self, layer_sizes, learning_rate = 0.1, friction = 1):\n",
    "        np.random.seed(0)\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.friction = friction\n",
    "\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        self.velocity_w = []\n",
    "        self.velocity_b = []\n",
    "\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "\n",
    "            # initialize random weights and biases\n",
    "            w = np.random.randn(layer_sizes[i], layer_sizes[i + 1])\n",
    "            b = np.zeros((1, layer_sizes[i + 1]))\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "\n",
    "            self.velocity_w.append(np.zeros_like(w))\n",
    "            self.velocity_b.append(np.zeros_like(b))\n",
    "\n",
    "    # activation function\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # activation function gradient (for backpropagation)\n",
    "    def sigmoid_gradient(self, x):\n",
    "        sx = self.sigmoid(x)\n",
    "        return sx * (1 - sx)\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        self.activations = [X] # activation - input matrix\n",
    "        self.z_values = [] # z_values - intermediate matrix before activation (sigmoid) is applied\n",
    "\n",
    "        activation = X\n",
    "        # perform matrix multiplication (xw + b)\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(activation, w) + b\n",
    "            self.z_values.append(z)\n",
    "            activation = self.sigmoid(z)\n",
    "            self.activations.append(activation) \n",
    "\n",
    "        return activation # final activated output (sigmoid(xw + b))\n",
    "\n",
    "    def backpropagation(self, X, y):\n",
    "        m = X.shape[0] # training data size\n",
    "        delta = self.activations[-1] - y # error (y_cap - y)\n",
    "\n",
    "        for i in range(len(self.weights) - 1, -1, -1): # this loop starts from the last layer, all the way to the first layer\n",
    "            dW = np.dot(self.activations[i].T, delta) / m # derivative of error w.r.t weights\n",
    "            db = np.sum(delta, axis=0, keepdims=True) / m # derivative of error w.r.t biases\n",
    "\n",
    "            if i > 0: # we cannnot include the input however\n",
    "                delta = np.dot(delta, self.weights[i].T) * self.sigmoid_gradient(self.z_values[i - 1]) # error update \n",
    "\n",
    "            #####################################################################################\n",
    "            # MOMENTUM (averaging is partially caused due to the friction element. (check notes for in-depth understanding))\n",
    "\n",
    "            # velocity_w_1 = (friction * velocity_w_0) + (l_r * dL/dW)\n",
    "            self.velocity_w[i] = (self.friction * self.velocity_w[i]) + (self.learning_rate * dW)\n",
    "            # velocity_b_1 = (friction * velocity_b_0) + (l_r * dL/db)\n",
    "            self.velocity_b[i] = (self.friction * self.velocity_b[i]) + (self.learning_rate * db)\n",
    "\n",
    "            self.weights[i] -= self.velocity_w[i]\n",
    "            self.biases[i] -= self.velocity_b[i]\n",
    "            \n",
    "            #####################################################################################\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        losses = []\n",
    "        epoch = 0\n",
    "        loss = 1e-8\n",
    "        while loss >= 1e-8:\n",
    "            output = self.forward_propagation(X) # perform forward propagation\n",
    "            loss = np.mean(np.square(output - y)) # calculate mean squared error loss\n",
    "            losses.append(loss)\n",
    "            self.backpropagation(X, y) # perform backpropagation   \n",
    "            epoch += 1        \n",
    "        print(f\"Total number of epochs: {epoch}\\nFinal loss: {loss.round(10)}\")\n",
    "\n",
    "# run neural network with gradient descent\n",
    "X = np.array([[5, 4, 2, -3],\n",
    "              [3, 1, -5, 0],\n",
    "              [9, -9, 3, 9],\n",
    "              [3, 1, -5, 0]])\n",
    "y = np.array([[1],\n",
    "              [0],\n",
    "              [0],\n",
    "              [0]])\n",
    "\n",
    "nn = NNMomentum((4, 5, 5, 12, 1, 22, 2))\n",
    "nn.train(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN with Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:**<br>\n",
    "1. Learning rate shrinks at each time step. \n",
    "2. Initial learning rate is large as the initial value at the denominator is small.\n",
    "3. As the gradients are accumulated, the denominator grows which leads to learning rate reduction.\n",
    "4. **Drawback**: If the network is very large, over accumulation of gradients will make the denominator very large leading to extremely slow updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of epochs: 13697\n",
      "Final loss: 1e-08\n"
     ]
    }
   ],
   "source": [
    "class NNAdagrad:\n",
    "    def __init__(self, layer_sizes, learning_rate = 0.1, epsilon = 1e-8):\n",
    "        np.random.seed(0)\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        self.sq_grads_w = []\n",
    "        self.sq_grads_b = []\n",
    "\n",
    "        self.stuff_w = []\n",
    "        self.stuff_b = []\n",
    "\n",
    "\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "\n",
    "            # initialize random weights and biases\n",
    "            w = np.random.randn(layer_sizes[i], layer_sizes[i + 1])\n",
    "            b = np.zeros((1, layer_sizes[i + 1]))\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "\n",
    "            self.sq_grads_w.append(np.zeros_like(w))\n",
    "            self.sq_grads_b.append(np.zeros_like(b))\n",
    "\n",
    "            self.stuff_w.append(np.zeros_like(w))\n",
    "            self.stuff_b.append(np.zeros_like(b))\n",
    "\n",
    "    # activation function\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # activation function gradient (for backpropagation)\n",
    "    def sigmoid_gradient(self, x):\n",
    "        sx = self.sigmoid(x)\n",
    "        return sx * (1 - sx)\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        self.activations = [X] # activation - input matrix\n",
    "        self.z_values = [] # z_values - intermediate matrix before activation (sigmoid) is applied\n",
    "\n",
    "        activation = X\n",
    "        # perform matrix multiplication (xw + b)\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(activation, w) + b\n",
    "            self.z_values.append(z)\n",
    "            activation = self.sigmoid(z)\n",
    "            self.activations.append(activation) \n",
    "\n",
    "        return activation # final activated output (sigmoid(xw + b))\n",
    "\n",
    "    def backpropagation(self, X, y):\n",
    "        m = X.shape[0] # training data size\n",
    "        delta = self.activations[-1] - y # error (y_cap - y)\n",
    "\n",
    "        for i in range(len(self.weights) - 1, -1, -1): # this loop starts from the last layer, all the way to the first layer\n",
    "            dW = np.dot(self.activations[i].T, delta) / m # derivative of error w.r.t weights\n",
    "            db = np.sum(delta, axis=0, keepdims=True) / m # derivative of error w.r.t biases\n",
    "\n",
    "            if i > 0: # we cannnot include the input however\n",
    "                delta = np.dot(delta, self.weights[i].T) * self.sigmoid_gradient(self.z_values[i - 1]) # error update \n",
    "\n",
    "            ###########################################################################################\n",
    "            # ADAGRAD\n",
    "            \n",
    "            # sum of squared gradients\n",
    "            self.sq_grads_w[i] += np.power(dW, 2)\n",
    "            self.sq_grads_b[i] += np.power(db, 2)\n",
    "            \n",
    "            # updation\n",
    "            self.weights[i] -= self.learning_rate * (dW / (np.sqrt(self.sq_grads_w[i]) + self.epsilon))\n",
    "            self.biases[i] -= self.learning_rate * (db / (np.sqrt(self.sq_grads_b[i]) + self.epsilon))\n",
    "            \n",
    "            ###########################################################################################\n",
    "\n",
    "    def train(self, X, y):\n",
    "        losses = []\n",
    "        epoch = 0\n",
    "        loss = 1e-8\n",
    "        while loss >= 1e-8:\n",
    "            output = self.forward_propagation(X) # perform forward propagation\n",
    "            loss = np.mean(np.square(output - y)) # calculate mean squared error loss\n",
    "            losses.append(loss)\n",
    "            self.backpropagation(X, y) # perform backpropagation   \n",
    "            epoch += 1        \n",
    "        print(f\"Total number of epochs: {epoch}\\nFinal loss: {loss.round(10)}\")\n",
    "\n",
    "# run neural network with gradient descent\n",
    "X = np.array([[5, 4, 2, -3],\n",
    "              [3, 1, -5, 0],\n",
    "              [9, -9, 3, 9],\n",
    "              [3, 1, -5, 0]])\n",
    "y = np.array([[1],\n",
    "              [0],\n",
    "              [0],\n",
    "              [0]])\n",
    "\n",
    "nn = NNAdagrad((4, 5, 5, 12, 1, 22, 2))\n",
    "nn.train(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN with RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:**<br>\n",
    "1. The issue where the derivatives kept gowring at every step without any limit due to the squaring of the gradient is eliminated her.\n",
    "2. This is done by multiplying `(1 - beta)` to `dw_squared`.\n",
    "\n",
    "__Testing:__<br>\n",
    "1. Why not take mod of the gradients instead of square?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of epochs: 40\n",
      "Final loss: 6.7e-09\n"
     ]
    }
   ],
   "source": [
    "class NNRMSProp:\n",
    "    def __init__(self, layer_sizes, learning_rate = 0.1, decay = 0.3, epsilon = 1e-8):\n",
    "        np.random.seed(0)\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        self.sq_grads_w = []\n",
    "        self.sq_grads_b = []\n",
    "\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "\n",
    "            # initialize random weights and biases\n",
    "            w = np.random.randn(layer_sizes[i], layer_sizes[i + 1])\n",
    "            b = np.zeros((1, layer_sizes[i + 1]))\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "\n",
    "            self.sq_grads_w.append(np.zeros_like(w))\n",
    "            self.sq_grads_b.append(np.zeros_like(b))\n",
    "\n",
    "    # activation function\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # activation function gradient (for backpropagation)\n",
    "    def sigmoid_gradient(self, x):\n",
    "        sx = self.sigmoid(x)\n",
    "        return sx * (1 - sx)\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        self.activations = [X] # activation - input matrix\n",
    "        self.z_values = [] # z_values - intermediate matrix before activation (sigmoid) is applied\n",
    "\n",
    "        activation = X\n",
    "        # perform matrix multiplication (xw + b)\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(activation, w) + b\n",
    "            self.z_values.append(z)\n",
    "            activation = self.sigmoid(z)\n",
    "            self.activations.append(activation) \n",
    "\n",
    "        return activation # final activated output (sigmoid(xw + b))\n",
    "\n",
    "    def backpropagation(self, X, y):\n",
    "        m = X.shape[0] # training data size\n",
    "        delta = self.activations[-1] - y # error (y_cap - y)\n",
    "\n",
    "        for i in range(len(self.weights) - 1, -1, -1): # this loop starts from the last layer, all the way to the first layer\n",
    "            dW = np.dot(self.activations[i].T, delta) / m # derivative of error w.r.t weights\n",
    "            db = np.sum(delta, axis=0, keepdims=True) / m # derivative of error w.r.t biases\n",
    "\n",
    "            if i > 0: # we cannnot include the input however\n",
    "                delta = np.dot(delta, self.weights[i].T) * self.sigmoid_gradient(self.z_values[i - 1]) # error update \n",
    "\n",
    "            ###########################################################################################\n",
    "            # RMSProp\n",
    "\n",
    "            # decay parameter lowers the effect of the squaring of the derivatives            \n",
    "            self.sq_grads_w[i] = self.decay * self.sq_grads_w[i] + (1 - self.decay) * np.power(dW, 2)\n",
    "            self.sq_grads_b[i] = self.decay * self.sq_grads_b[i] + (1 - self.decay) * np.power(db, 2)\n",
    "\n",
    "            self.weights[i] -= self.learning_rate * (dW / (np.sqrt(self.sq_grads_w[i]) + self.epsilon))\n",
    "            self.biases[i] -= self.learning_rate * (db / (np.sqrt(self.sq_grads_b[i]) + self.epsilon))\n",
    "\n",
    "            # ADAGRAD (FOR COMPARISON)\n",
    "            \n",
    "            # self.sq_grads_w[i] += np.power(dW, 2)\n",
    "            # self.sq_grads_b[i] += np.power(db, 2)\n",
    "\n",
    "            # self.weights[i] -= self.learning_rate * (dW / (np.sqrt(self.sq_grads_w[i]) + self.epsilon))\n",
    "            # self.biases[i] -= self.learning_rate * (db / (np.sqrt(self.sq_grads_b[i]) + self.epsilon))\n",
    "            \n",
    "            #############################################################################################\n",
    "\n",
    "    def train(self, X, y):\n",
    "        losses = []\n",
    "        epoch = 0\n",
    "        loss = 1e-8\n",
    "        while loss >= 1e-8:\n",
    "            output = self.forward_propagation(X) # perform forward propagation\n",
    "            loss = np.mean(np.square(output - y)) # calculate mean squared error loss\n",
    "            losses.append(loss)\n",
    "            self.backpropagation(X, y) # perform backpropagation   \n",
    "            epoch += 1        \n",
    "        print(f\"Total number of epochs: {epoch}\\nFinal loss: {loss.round(10)}\")\n",
    "\n",
    "# run neural network with gradient descent\n",
    "X = np.array([[5, 4, 2, -3],\n",
    "              [3, 1, -5, 0],\n",
    "              [9, -9, 3, 9],\n",
    "              [3, 1, -5, 0]])\n",
    "y = np.array([[1],\n",
    "              [0],\n",
    "              [0],\n",
    "              [0]])\n",
    "\n",
    "nn = NNRMSProp((4, 5, 5, 12, 1, 22, 2))\n",
    "nn.train(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN with Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:**<br>\n",
    "1. Here Momentum & RMSProp is combined.\n",
    "2. RMSProp stays the same while the Momentum formula is slighlty tweaked for normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of epochs: 30\n",
      "Final loss: 1.3e-09\n"
     ]
    }
   ],
   "source": [
    "class NNAdam:\n",
    "    def __init__(self, layer_sizes, learning_rate = 0.1, delta_one = 0.8, delta_two = 0.5, epsilon = 1e-8):\n",
    "        np.random.seed(0)\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.delta_one = delta_one\n",
    "        self.delta_two = delta_two\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        self.moment_one_w = []\n",
    "        self.moment_one_b = []\n",
    "        self.moment_two_w = []\n",
    "        self.moment_two_b = []\n",
    "\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "\n",
    "            # initialize random weights and biases\n",
    "            w = np.random.randn(layer_sizes[i], layer_sizes[i + 1])\n",
    "            b = np.zeros((1, layer_sizes[i + 1]))\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "\n",
    "            self.moment_one_w.append(np.zeros_like(w))\n",
    "            self.moment_one_b.append(np.zeros_like(b))\n",
    "            self.moment_two_w.append(np.zeros_like(w))\n",
    "            self.moment_two_b.append(np.zeros_like(b))\n",
    "\n",
    "    # activation function\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # activation function gradient (for backpropagation)\n",
    "    def sigmoid_gradient(self, x):\n",
    "        sx = self.sigmoid(x)\n",
    "        return sx * (1 - sx)\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        self.activations = [X] # activation - input matrix\n",
    "        self.z_values = [] # z_values - intermediate matrix before activation (sigmoid) is applied\n",
    "\n",
    "        activation = X\n",
    "        # perform matrix multiplication (xw + b)\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(activation, w) + b\n",
    "            self.z_values.append(z)\n",
    "            activation = self.sigmoid(z)\n",
    "            self.activations.append(activation) \n",
    "\n",
    "        return activation # final activated output (sigmoid(xw + b))\n",
    "\n",
    "    def backpropagation(self, X, y, epoch):\n",
    "        m = X.shape[0] # training data size\n",
    "        delta = self.activations[-1] - y # error (y_cap - y)\n",
    "\n",
    "        for i in range(len(self.weights) - 1, -1, -1): # this loop starts from the last layer, all the way to the first layer\n",
    "            dW = np.dot(self.activations[i].T, delta) / m # derivative of error w.r.t weights\n",
    "            db = np.sum(delta, axis=0, keepdims=True) / m # derivative of error w.r.t biases\n",
    "\n",
    "            if i > 0: # we cannnot include the input however\n",
    "                delta = np.dot(delta, self.weights[i].T) * self.sigmoid_gradient(self.z_values[i - 1]) # error update \n",
    "\n",
    "            ###################################################################################################################\n",
    "            # ADAM\n",
    "\n",
    "            # MOMENTUM (FOR COMPARISON)\n",
    "\n",
    "            # self.velocity_w[i] = (self.friction * self.velocity_w[i]) + (self.learning_rate * dW)\n",
    "            # self.velocity_b[i] = (self.friction * self.velocity_b[i]) + (self.learning_rate * db)\n",
    "\n",
    "            # self.weights[i] -= self.velocity_w[i]\n",
    "            # self.biases[i] -= self.velocity_b[i]\n",
    "\n",
    "            # this is momentum\n",
    "            self.moment_one_w[i] = self.delta_one * self.moment_one_w[i] + (1 - self.delta_one) * dW # moment one\n",
    "            self.moment_one_b[i] = self.delta_one * self.moment_one_b[i] + (1 - self.delta_one) * db # moment one\n",
    "\n",
    "            # this is rmsprop\n",
    "            self.moment_two_w[i] = self.delta_two * self.moment_two_w[i] + (1 - self.delta_two) * np.power(dW, 2) # moment two\n",
    "            self.moment_two_b[i] = self.delta_two * self.moment_two_b[i] + (1 - self.delta_two) * np.power(db, 2) # moment two\n",
    " \n",
    "            # bias correction\n",
    "            moment_one_w_hat = self.moment_one_w[i] / (1 - np.power(self.delta_one, epoch + 1))\n",
    "            moment_one_b_hat = self.moment_one_b[i] / (1 - np.power(self.delta_one, epoch + 1))\n",
    "\n",
    "            moment_two_w_hat = self.moment_two_w[i] / (1 - np.power(self.delta_two, epoch + 1))\n",
    "            moment_two_b_hat = self.moment_two_b[i] / (1 - np.power(self.delta_two, epoch + 1))\n",
    "\n",
    "            # parameters update\n",
    "            self.weights[i] -= self.learning_rate * (moment_one_w_hat / (np.sqrt(moment_two_w_hat) + self.epsilon))\n",
    "            self.biases[i] -= self.learning_rate * (moment_one_b_hat / (np.sqrt(moment_two_b_hat) + self.epsilon))\n",
    "\n",
    "            ###################################################################################################################\n",
    "\n",
    "    def train(self, X, y):\n",
    "        losses = []\n",
    "        epoch = 0\n",
    "        loss = 1e-8\n",
    "        while loss >= 1e-8:\n",
    "            output = self.forward_propagation(X) # perform forward propagation\n",
    "            loss = np.mean(np.square(output - y)) # calculate mean squared error loss\n",
    "            losses.append(loss)\n",
    "            self.backpropagation(X, y, epoch) # perform backpropagation   \n",
    "            epoch += 1        \n",
    "        print(f\"Total number of epochs: {epoch}\\nFinal loss: {loss.round(10)}\")\n",
    "\n",
    "# run neural network with gradient descent\n",
    "X = np.array([[5, 4, 2, -3],\n",
    "              [3, 1, -5, 0],\n",
    "              [9, -9, 3, 9],\n",
    "              [3, 1, -5, 0]])\n",
    "y = np.array([[1],\n",
    "              [0],\n",
    "              [0],\n",
    "              [0]])\n",
    "\n",
    "nn = NNAdam((4, 5, 5, 12, 1, 22, 2))\n",
    "nn.train(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN with AdaMax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:**<br>\n",
    "1. With Adam, when calculating the second moment estimate, we sort of calculate the L2 norm of the gradients. \n",
    "2. With AdaMax, instead of squaring dW, we take the maximum absolute value between the current gradient and the past gradients, which is decayed using the decay factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of epochs: 28\n",
      "Final loss: 7.7e-09\n"
     ]
    }
   ],
   "source": [
    "class NNAdaMax:\n",
    "    def __init__(self, layer_sizes, learning_rate = 0.1, delta_one = 0.7, delta_two = 0.7, inf_norm_w = 1e-8, inf_norm_b = 1e-8, epsilon = 1e-8):\n",
    "        np.random.seed(0)\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.delta_one = delta_one\n",
    "        self.delta_two = delta_two\n",
    "        self.inf_norm_w = inf_norm_w\n",
    "        self.inf_norm_b = inf_norm_b\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        self.moment_one_w = []\n",
    "        self.moment_one_b = []\n",
    "\n",
    "        self.inf_norm_w = []\n",
    "        self.inf_norm_b = []\n",
    "\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "\n",
    "            # initialize random weights and biases\n",
    "            w = np.random.randn(layer_sizes[i], layer_sizes[i + 1])\n",
    "            b = np.zeros((1, layer_sizes[i + 1]))\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "\n",
    "            self.moment_one_w.append(np.zeros_like(w))\n",
    "            self.moment_one_b.append(np.zeros_like(b))\n",
    "\n",
    "            self.inf_norm_w.append(np.zeros_like(w))\n",
    "            self.inf_norm_b.append(np.zeros_like(b))\n",
    "\n",
    "    # activation function\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # activation function gradient (for backpropagation)\n",
    "    def sigmoid_gradient(self, x):\n",
    "        sx = self.sigmoid(x)\n",
    "        return sx * (1 - sx)\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        self.activations = [X] # activation - input matrix\n",
    "        self.z_values = [] # z_values - intermediate matrix before activation (sigmoid) is applied\n",
    "\n",
    "        activation = X\n",
    "        # perform matrix multiplication (xw + b)\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(activation, w) + b\n",
    "            self.z_values.append(z)\n",
    "            activation = self.sigmoid(z)\n",
    "            self.activations.append(activation) \n",
    "\n",
    "        return activation # final activated output (sigmoid(xw + b))\n",
    "\n",
    "    def backpropagation(self, X, y, epoch):\n",
    "        m = X.shape[0] # training data size\n",
    "        delta = self.activations[-1] - y # error (y_cap - y)\n",
    "\n",
    "        for i in range(len(self.weights) - 1, -1, -1): # this loop starts from the last layer, all the way to the first layer\n",
    "            dW = np.dot(self.activations[i].T, delta) / m # derivative of error w.r.t weights\n",
    "            db = np.sum(delta, axis=0, keepdims=True) / m # derivative of error w.r.t biases\n",
    "\n",
    "            if i > 0: # we cannnot include the input however\n",
    "                delta = np.dot(delta, self.weights[i].T) * self.sigmoid_gradient(self.z_values[i - 1]) # error update \n",
    "\n",
    "            ################################################################################################\n",
    "            # ADAMAX\n",
    "\n",
    "            # first moment estimate\n",
    "            self.moment_one_w[i] = self.delta_one * self.moment_one_w[i] + (1 - self.delta_one) * dW\n",
    "            self.moment_one_b[i] = self.delta_one * self.moment_one_b[i] + (1 - self.delta_one) * db\n",
    "\n",
    "            # infinity norm (max norm of past gradients instead of squaring)\n",
    "            self.inf_norm_w[i] = np.maximum(self.delta_two * self.inf_norm_w[i], np.abs(dW))\n",
    "            self.inf_norm_b[i] = np.maximum(self.delta_two * self.inf_norm_b[i], np.abs(db))\n",
    "\n",
    "            # bias correction\n",
    "            moment_one_w_hat = self.moment_one_w[i] / (1 - (np.power(self.delta_one, epoch + 1)))\n",
    "            moment_one_b_hat = self.moment_one_b[i] / (1 - (np.power(self.delta_one, epoch + 1)))\n",
    "\n",
    "            # parameters update\n",
    "            self.weights[i] -= (self.learning_rate * moment_one_w_hat) / (self.inf_norm_w[i] + self.epsilon)\n",
    "            self.biases[i] -= (self.learning_rate * moment_one_b_hat) / (self.inf_norm_b[i] + self.epsilon)\n",
    "            \n",
    "            ################################################################################################\n",
    "\n",
    "    def train(self, X, y):\n",
    "        losses = []\n",
    "        epoch = 0\n",
    "        loss = 1e-8\n",
    "        while loss >= 1e-8:\n",
    "            output = self.forward_propagation(X) # perform forward propagation\n",
    "            loss = np.mean(np.square(output - y)) # calculate mean squared error loss\n",
    "            losses.append(loss)\n",
    "            self.backpropagation(X, y, epoch) # perform backpropagation   \n",
    "            epoch += 1        \n",
    "        print(f\"Total number of epochs: {epoch}\\nFinal loss: {loss.round(10)}\")\n",
    "\n",
    "# run neural network with gradient descent\n",
    "X = np.array([[5, 4, 2, -3],\n",
    "              [3, 1, -5, 0],\n",
    "              [9, -9, 3, 9],\n",
    "              [3, 1, -5, 0]])\n",
    "y = np.array([[1],\n",
    "              [0],\n",
    "              [0],\n",
    "              [0]])\n",
    "\n",
    "nn = NNAdaMax((4, 5, 5, 12, 1, 22, 2))\n",
    "nn.train(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN with AdaBelief"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:**<br>\n",
    "1. In AdaBelief, we tweak the original Adam algorithm, yet again with the second moment estimate. \n",
    "2. We subtract the gradient with alpha from the previous time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of epochs: 23\n",
      "Final loss: 8.1e-09\n"
     ]
    }
   ],
   "source": [
    "class NNAdaBelief:\n",
    "    def __init__(self, layer_sizes, learning_rate = 0.1, delta_one = 0.4, delta_two = 0.2, epsilon = 1e-8):\n",
    "        np.random.seed(0)\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.delta_one = delta_one\n",
    "        self.delta_two = delta_two\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        self.moment_one_w = []\n",
    "        self.moment_one_b = []\n",
    "        self.moment_two_w = []\n",
    "        self.moment_two_b = []\n",
    "\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "\n",
    "            # initialize random weights and biases\n",
    "            w = np.random.randn(layer_sizes[i], layer_sizes[i + 1])\n",
    "            b = np.zeros((1, layer_sizes[i + 1]))\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "\n",
    "            self.moment_one_w.append(np.zeros_like(w))\n",
    "            self.moment_one_b.append(np.zeros_like(b))\n",
    "            self.moment_two_w.append(np.zeros_like(w))\n",
    "            self.moment_two_b.append(np.zeros_like(b))\n",
    "\n",
    "    # activation function\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # activation function gradient (for backpropagation)\n",
    "    def sigmoid_gradient(self, x):\n",
    "        sx = self.sigmoid(x)\n",
    "        return sx * (1 - sx)\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        self.activations = [X] # activation - input matrix\n",
    "        self.z_values = [] # z_values - intermediate matrix before activation (sigmoid) is applied\n",
    "\n",
    "        activation = X\n",
    "        # perform matrix multiplication (xw + b)\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(activation, w) + b\n",
    "            self.z_values.append(z)\n",
    "            activation = self.sigmoid(z)\n",
    "            self.activations.append(activation) \n",
    "\n",
    "        return activation # final activated output (sigmoid(xw + b))\n",
    "\n",
    "    def backpropagation(self, X, y, epoch):\n",
    "        m = X.shape[0] # training data size\n",
    "        delta = self.activations[-1] - y # error (y_cap - y)\n",
    "\n",
    "        for i in range(len(self.weights) - 1, -1, -1): # this loop starts from the last layer, all the way to the first layer\n",
    "            dW = np.dot(self.activations[i].T, delta) / m # derivative of error w.r.t weights\n",
    "            db = np.sum(delta, axis=0, keepdims=True) / m # derivative of error w.r.t biases\n",
    "\n",
    "            if i > 0: # we cannnot include the input however\n",
    "                delta = np.dot(delta, self.weights[i].T) * self.sigmoid_gradient(self.z_values[i - 1]) # error update \n",
    "\n",
    "            ############################################################################################################################\n",
    "            # ADABELIEF\n",
    "\n",
    "            # moments calculation\n",
    "            self.moment_one_w[i] = self.delta_one * self.moment_one_w[i] + (1 - self.delta_one) * dW\n",
    "            self.moment_one_b[i] = self.delta_one * self.moment_one_b[i] + (1 - self.delta_one) * db\n",
    "\n",
    "            self.moment_two_w[i] = self.delta_two * self.moment_two_w[i] + (1 - self.delta_two) * np.power(dW - self.moment_two_w[i], 2)\n",
    "            self.moment_two_b[i] = self.delta_two * self.moment_two_b[i] + (1 - self.delta_two) * np.power(db - self.moment_two_b[i], 2)\n",
    "\n",
    "            # bias correction\n",
    "            moment_one_w_hat = self.moment_one_w[i] / (1 - np.power(self.delta_one, epoch + 1))\n",
    "            moment_one_b_hat = self.moment_one_b[i] / (1 - np.power(self.delta_one, epoch + 1))\n",
    "\n",
    "            moment_two_w_hat = self.moment_two_w[i] / (1 - np.power(self.delta_two, epoch + 1))\n",
    "            moment_two_b_hat = self.moment_two_b[i] / (1 - np.power(self.delta_two, epoch + 1))\n",
    "\n",
    "            # parameters update\n",
    "            self.weights[i] -= self.learning_rate * (moment_one_w_hat / (np.sqrt(moment_two_w_hat) + self.epsilon))\n",
    "            self.biases[i] -= self.learning_rate * (moment_one_b_hat / (np.sqrt(moment_two_b_hat) + self.epsilon))\n",
    "\n",
    "            ############################################################################################################################\n",
    "\n",
    "    def train(self, X, y):\n",
    "        losses = []\n",
    "        epoch = 0\n",
    "        loss = 1e-8\n",
    "        while loss >= 1e-8:\n",
    "            output = self.forward_propagation(X) # perform forward propagation\n",
    "            loss = np.mean(np.square(output - y)) # calculate mean squared error loss\n",
    "            losses.append(loss)\n",
    "            self.backpropagation(X, y, epoch) # perform backpropagation   \n",
    "            epoch += 1        \n",
    "        print(f\"Total number of epochs: {epoch}\\nFinal loss: {loss.round(10)}\")\n",
    "\n",
    "# run neural network with gradient descent\n",
    "X = np.array([[5, 4, 2, -3],\n",
    "              [3, 1, -5, 0],\n",
    "              [9, -9, 3, 9],\n",
    "              [3, 1, -5, 0]])\n",
    "y = np.array([[1],\n",
    "              [0],\n",
    "              [0],\n",
    "              [0]])\n",
    "\n",
    "nn = NNAdaBelief((4, 5, 5, 12, 1, 22, 2))\n",
    "nn.train(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN with Nadam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:**<br>\n",
    "1. Nadam is another modification to Adam. \n",
    "2. The only difference is that instead of the classic Momentum formula, we replace it with Nesterov Momentum. \n",
    "3. This has proven to work better with very noisy gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of epochs: 18\n",
      "Final loss: 0.0007338104\n"
     ]
    }
   ],
   "source": [
    "class NNNadam:\n",
    "    def __init__(self, layer_sizes, learning_rate = 0.1, delta_one = 0.6, delta_two = 0.5, epsilon = 1e-8):\n",
    "        np.random.seed(0)\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.delta_one = delta_one\n",
    "        self.delta_two = delta_two\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        self.moment_one_w = []\n",
    "        self.moment_one_b = []\n",
    "        self.moment_two_w = []\n",
    "        self.moment_two_b = []\n",
    "\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "\n",
    "            # initialize random weights and biases\n",
    "            w = np.random.randn(layer_sizes[i], layer_sizes[i + 1])\n",
    "            b = np.zeros((1, layer_sizes[i + 1]))\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "\n",
    "            self.moment_one_w.append(np.zeros_like(w))\n",
    "            self.moment_one_b.append(np.zeros_like(b))\n",
    "            self.moment_two_w.append(np.zeros_like(w))\n",
    "            self.moment_two_b.append(np.zeros_like(b))\n",
    "\n",
    "    # activation function\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # activation function gradient (for backpropagation)\n",
    "    def sigmoid_gradient(self, x):\n",
    "        sx = self.sigmoid(x)\n",
    "        return sx * (1 - sx)\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        self.activations = [X] # activation - input matrix\n",
    "        self.z_values = [] # z_values - intermediate matrix before activation (sigmoid) is applied\n",
    "\n",
    "        activation = X\n",
    "        # perform matrix multiplication (xw + b)\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(activation, w) + b\n",
    "            self.z_values.append(z)\n",
    "            activation = self.sigmoid(z)\n",
    "            self.activations.append(activation) \n",
    "\n",
    "        return activation # final activated output (sigmoid(xw + b))\n",
    "\n",
    "    def backpropagation(self, X, y, epoch):\n",
    "        m = X.shape[0] # training data size\n",
    "        delta = self.activations[-1] - y # error (y_cap - y)\n",
    "\n",
    "        for i in range(len(self.weights) - 1, -1, -1): # this loop starts from the last layer, all the way to the first layer\n",
    "            dW = np.dot(self.activations[i].T, delta) / m # derivative of error w.r.t weights\n",
    "            db = np.sum(delta, axis=0, keepdims=True) / m # derivative of error w.r.t biases\n",
    "\n",
    "            if i > 0: # we cannnot include the input however\n",
    "                delta = np.dot(delta, self.weights[i].T) * self.sigmoid_gradient(self.z_values[i - 1]) # error update \n",
    "\n",
    "            ########################################################################################################\n",
    "            # NESTROV-ACCELERATED ADAM (NADAM)\n",
    "\n",
    "            # moments calculation\n",
    "            self.moment_one_w[i] = self.delta_one * self.moment_one_w[i] + (1 - self.delta_one) * dW\n",
    "            self.moment_one_b[i] = self.delta_one * self.moment_one_b[i] + (1 - self.delta_one) * db\n",
    "\n",
    "            self.moment_two_w[i] = self.delta_two * self.moment_two_w[i] + (1 - self.delta_two) * np.power(dW, 2)\n",
    "            self.moment_two_b[i] = self.delta_two * self.moment_two_b[i] + (1 - self.delta_two) * np.power(db, 2)\n",
    "\n",
    "            # bias correction\n",
    "            moment_one_w_hat = self.moment_one_w[i] / (1 - np.power(self.delta_one, epoch + 1))\n",
    "            moment_one_b_hat = self.moment_one_b[i] / (1 - np.power(self.delta_one, epoch + 1))\n",
    "\n",
    "            moment_two_w_hat = self.moment_two_w[i] / (1 - np.power(self.delta_two, epoch + 1))\n",
    "            moment_two_b_hat = self.moment_two_b[i] / (1 - np.power(self.delta_two, epoch + 1))\n",
    "\n",
    "            # nesterov lookahead adjustment\n",
    "            moment_one_w_star = (1 - self.delta_one) * dW + self.delta_one * moment_one_w_hat\n",
    "            moment_one_b_star = (1 - self.delta_one) * db + self.delta_one * moment_one_b_hat\n",
    "\n",
    "            # parameters update\n",
    "            self.weights[i] -= self.learning_rate * (moment_one_w_star / (np.sqrt(moment_two_w_hat) + self.epsilon))\n",
    "            self.biases[i] -= self.learning_rate * (moment_one_b_star / (np.sqrt(moment_two_b_hat) + self.epsilon))\n",
    "\n",
    "            ########################################################################################################\n",
    "\n",
    "    def train(self, X, y):\n",
    "        losses = []\n",
    "        epoch = 0\n",
    "        loss = 1e-3\n",
    "        while loss >= 1e-3:\n",
    "            output = self.forward_propagation(X) # perform forward propagation\n",
    "            loss = np.mean(np.square(output - y)) # calculate mean squared error loss\n",
    "            losses.append(loss)\n",
    "            self.backpropagation(X, y, epoch) # perform backpropagation   \n",
    "            epoch += 1        \n",
    "        print(f\"Total number of epochs: {epoch}\\nFinal loss: {loss.round(10)}\")\n",
    "\n",
    "# run neural network with gradient descent\n",
    "X = np.array([[5, 4, 2, -3],\n",
    "              [3, 1, -5, 0],\n",
    "              [9, -9, 3, 9],\n",
    "              [3, 1, -5, 0]])\n",
    "y = np.array([[1],\n",
    "              [0],\n",
    "              [0],\n",
    "              [0]])\n",
    "\n",
    "nn = NNNadam((4, 5, 5, 12, 1, 22, 2))\n",
    "nn.train(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The reduction in steps to converge from 73258 steps (gradient descent) to 18 steps (nadam) is amazing!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
